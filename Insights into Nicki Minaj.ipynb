{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ff79ad",
   "metadata": {},
   "source": [
    "# Project 1.2 : Insights into Nicki Minaj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3caef",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b207551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Usuari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# importing NLTK functions for part-of-speech tagging and regex-based parsing\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# importing function that returns most common noun phrases and verb phrases chunks\n",
    "from chunk_counters import np_chunk_counter, vp_chunk_counter\n",
    "# importing function that tokenizes text input firstly into sentences, then into words\n",
    "from tokenize_words import word_sentence_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# importing the song Chun-Li from Nicki Minaj, lowering the case for processing\n",
    "text = open('nicki_minaj.txt', encoding = 'utf-8').read().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc47040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence and word tokenizing text in order to perform sentence-by-sentence parsing analysis later.\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5ce7fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now', 'everybody', 'like', ',', '``', 'she', 'really', 'is', 'the', 'best', \"''\", 'you', 'play', 'checkers', ',', 'could', \"n't\", 'beat', 'me', 'playin', \"'\", 'chess', 'now', 'i', \"'m\", 'about', 'to', 'turn', 'around', 'and', 'beat', 'my', 'chest', 'bitch', ',', 'it', \"'s\", 'king', 'kong', ',', 'yes', ',', 'it', \"'s\", 'king', 'kong', 'bitch', ',', 'it', \"'s\", 'king', 'kong', ',', 'this', 'is', 'kin', \"'\", 'kong', 'chinese', 'ink', 'on', ',', 'siamese', 'links', 'on', 'call', 'me', '2', 'chainz', ',', 'name', 'go', 'ding-dong', 'bitch', ',', 'it', \"'s\", 'king', 'kong', ',', 'yes', ',', 'i', \"'m\", 'king', 'kong', 'this', 'is', 'king', 'kong', '?']\n"
     ]
    }
   ],
   "source": [
    "# storing and printing a random index of word_tokenized_text to visualize what the function returns\n",
    "\n",
    "test_word_tokenized_sentence = word_tokenized_text[1]\n",
    "print(test_word_tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d1eb1",
   "metadata": {},
   "source": [
    "## Part-of-speech Tag Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0603deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an empty list to hold part-of-speech tagged sentences from the text under analysis.\n",
    "pos_tagged_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b39a4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a for loop through each word tokenized sentence (token) and append to list of pos tagged text using nltk's pos_tag() function.\n",
    "for token in word_tokenized_text:\n",
    "    pos_tagged_text.append(pos_tag(token))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8af95d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('six', 'CD'), ('rings', 'NNS'), ('on', 'IN'), ('they', 'PRP'), ('need', 'VBP'), ('rappers', 'NNS'), ('like', 'IN'), ('me', 'PRP'), ('they', 'PRP'), ('need', 'VBP'), ('rappers', 'NNS'), ('like', 'IN'), ('me', 'PRP'), ('so', 'IN'), ('they', 'PRP'), ('can', 'MD'), ('get', 'VB'), ('on', 'IN'), ('their', 'PRP$'), ('fuckin', 'NN'), (\"'\", \"''\"), ('keyboards', 'NNS'), ('and', 'CC'), ('make', 'VB'), ('me', 'PRP'), (',', ','), ('the', 'DT'), ('bad', 'JJ'), ('guy', 'NN'), ('chun-li', 'JJ'), ('ayy', 'NN'), (',', ','), ('yo', 'NN'), (',', ','), ('i', 'RB'), ('been', 'VBN'), ('on', 'IN'), (',', ','), ('bitch', 'NN'), (',', ','), ('you', 'PRP'), ('been', 'VBN'), ('corn', 'NN'), ('bentley', 'NN'), ('tints', 'NNS'), ('on', 'IN'), (',', ','), ('fendi', 'JJ'), ('prints', 'NNS'), ('on', 'IN'), ('i', 'JJ'), ('mean', 'VBP'), ('i', 'VBP'), ('been', 'VBN'), ('storm', 'JJ'), (',', ','), ('x-men', 'JJ'), ('been', 'VBN'), ('formed', 'VBN'), ('he', 'PRP'), ('keep', 'VB'), ('on', 'IN'), ('dialin', 'NN'), (\"'\", \"''\"), ('nicki', 'RB'), (',', ','), ('like', 'IN'), ('the', 'DT'), ('prince', 'NN'), ('song', 'NN'), ('i-i-i', 'JJ'), ('been', 'VBN'), ('on', 'IN'), (',', ','), ('bitch', 'NN'), (',', ','), ('you', 'PRP'), ('been', 'VBN'), ('corn', 'NN'), ('bentley', 'NN'), ('tints', 'NNS'), ('on', 'IN'), (',', ','), ('fendi', 'JJ'), ('prints', 'NNS'), ('on', 'IN'), ('ayy', 'NN'), (',', ','), ('yo', 'NN'), (',', ','), ('i', 'VBP'), ('been', 'VBN'), ('north', 'JJ'), (',', ','), ('laura', 'JJ'), ('been', 'VBN'), ('croft', 'JJ'), ('plates', 'NNS'), ('say', 'VBP'), (\"'chun-li\", 'JJ'), (\"'\", \"''\"), ('drop', 'NN'), ('the', 'DT'), ('benz', 'NN'), ('off', 'RP'), (',', ','), ('uh', 'UH'), ('i', 'JJ'), ('come', 'VBP'), ('alive', 'JJ'), (',', ','), ('i', 'JJ'), (',', ','), ('i', 'JJ'), (\"'m\", 'VBP'), ('always', 'RB'), ('sky', 'RB'), ('high', 'JJ'), ('designer', 'NN'), ('thigh', 'JJ'), ('highs', 'NNS'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('my', 'PRP$'), ('lifestyle', 'JJ'), ('i', 'JJ'), ('come', 'VBP'), ('alive', 'JJ'), (',', ','), ('i', 'JJ'), (',', ','), ('i', 'JJ'), (\"'m\", 'VBP'), ('always', 'RB'), ('sky', 'RB'), ('high', 'JJ'), ('designer', 'NN'), ('thigh', 'JJ'), ('highs', 'NNS'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('my', 'PRP$'), ('lifestyle', 'JJ'), ('i', 'NNS'), ('need', 'VBP'), ('a', 'DT'), ('mai', 'NN'), ('tai', 'NN'), (',', ','), ('so', 'RB'), ('fuckin', 'JJ'), (\"'\", \"''\"), ('sci-fi', 'JJ'), ('give', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('password', 'NN'), (',', ','), ('to', 'TO'), ('the', 'DT'), ('fuckin', 'NN'), (\"'\", 'POS'), ('wifi', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# storing and printing any random part-of-speech tagged sentence to visualize single pos-tagged sentences\n",
    "test_pos_sentence = pos_tagged_text[4]\n",
    "print(test_pos_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be552549",
   "metadata": {},
   "source": [
    "## Chunk Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4333ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a noun phrase \n",
    "# (considered here as an optional determiner, any number of adjectives and an obligatory noun, in this order) chunk grammar \n",
    "np_chunk_grammar = 'NP: {<DT>?<JJ>*<NN>}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c985dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a noun phrase RegexpParser object \n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f807a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining verb phrase (here considered as noun phrase, a verb in any form and an optional adverb, in this order) chunk grammar\n",
    "vp_chunk_grammar = 'VP: {<DT>?<JJ>*<NN><VB.*><RB>?}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ee7f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating verb phrase RegexpParser object\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "047c8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists to hold noun phrase chunked sentences and verb phrase chunked sentences\n",
    "np_chunked_text = []\n",
    "vp_chunked_text = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "162d0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a for loop through each pos-tagged sentence which chunks each sentence and appends to list \n",
    "for pos_tagged in pos_tagged_text:\n",
    "    np_chunked_text.append(np_chunk_parser.parse(pos_tagged))\n",
    "    vp_chunked_text.append(vp_chunk_parser.parse(pos_tagged))\n",
    " \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7d0e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  yes/UH\n",
      "  ,/,\n",
      "  miss/JJ\n",
      "  king/NN\n",
      "  kong/NN\n",
      "  in/IN\n",
      "  my/PRP$\n",
      "  kingdom/NN\n",
      "  with/IN\n",
      "  my/PRP$\n",
      "  timbs/NNS\n",
      "  on/IN\n",
      "  (/(\n",
      "  how/WRB\n",
      "  many/JJ\n",
      "  championships/NNS\n",
      "  ?/.\n",
      "  )/))\n",
      "(S\n",
      "  yes/UH\n",
      "  ,/,\n",
      "  (NP miss/JJ king/NN)\n",
      "  (NP kong/NN)\n",
      "  in/IN\n",
      "  my/PRP$\n",
      "  (NP kingdom/NN)\n",
      "  with/IN\n",
      "  my/PRP$\n",
      "  timbs/NNS\n",
      "  on/IN\n",
      "  (/(\n",
      "  how/WRB\n",
      "  many/JJ\n",
      "  championships/NNS\n",
      "  ?/.\n",
      "  )/))\n"
     ]
    }
   ],
   "source": [
    "# visualizing random indexes of chunked sentences\n",
    "print(vp_chunked_text[2])  \n",
    "print(np_chunked_text[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade8062",
   "metadata": {},
   "source": [
    "## Analyze Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17916ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('bitch', 'NN'),), 6), ((('i', 'NN'),), 5), ((('ayy', 'NN'),), 4), ((('yo', 'NN'),), 4), ((('corn', 'NN'),), 4), ((('bentley', 'NN'),), 4), ((('dialin', 'NN'),), 2), ((('the', 'DT'), ('prince', 'NN')), 2), ((('song', 'NN'),), 2), ((('the', 'DT'), ('benz', 'NN')), 2), ((('the', 'DT'), ('bad', 'JJ'), ('guy', 'NN')), 2), ((('kong', 'NN'),), 2), ((('high', 'JJ'), ('designer', 'NN')), 2), ((('goin', 'NN'),), 1), ((('a', 'DT'), ('swim', 'NN')), 1), ((('swingin', 'NN'),), 1), ((('the', 'DT'), ('rim', 'JJ'), ('bitch', 'NN')), 1), ((('the', 'DT'), ('bench', 'NN')), 1), ((('the', 'DT'), ('court', 'NN')), 1), ((('some', 'DT'), ('haterade', 'NN')), 1), ((('thirst', 'JJ'), ('quenched', 'JJ'), ('style', 'NN')), 1), ((('this', 'DT'), ('burberry', 'NN')), 1), ((('every', 'DT'), ('word', 'NN')), 1), ((('every', 'DT'), ('inch', 'NN')), 1), ((('the', 'DT'), ('hammer', 'NN')), 1), ((('the', 'DT'), ('wrench', 'NN')), 1), ((('brrt', 'NN'),), 1), ((('that', 'DT'), ('quarter', 'NN')), 1), ((('the', 'DT'), ('lot', 'NN')), 1), ((('the', 'DT'), ('roc', 'NN')), 1)]\n"
     ]
    }
   ],
   "source": [
    "# storing and printing the most common NP-chunks in order to gain insights about the book's most relevant noun phrases\n",
    "# using the function np_chunk_counter()\n",
    "\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(most_common_np_chunks)\n",
    "\n",
    "# thanks to this process, we gain a clear insight about the song's main themes. \n",
    "# however, we can notice that the analysis is way less accurate than with a novel like The Iliad due to many interjections and less meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9cfdc4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'), (\"'m\", 'VBP')), 2), ((('a', 'DT'), ('swim', 'NN'), ('dunked', 'VBN')), 1), ((('the', 'DT'), ('rim', 'JJ'), ('bitch', 'NN'), ('ai', 'VBP'), (\"n't\", 'RB')), 1), ((('this', 'DT'), ('burberry', 'NN'), ('trench', 'VBZ')), 1), ((('i', 'NN'), ('pull', 'VBP')), 1), ((('that', 'DT'), ('quarter', 'NN'), ('milli', 'VBD')), 1), ((('i', 'NN'), ('forgot', 'VBD'), ('show', 'RB')), 1), ((('the', 'DT'), ('roc', 'NN'), ('ai', 'VBP'), (\"n't\", 'RB')), 1), ((('a', 'DT'), ('bad', 'JJ'), ('guy', 'NN'), ('do', 'VBP')), 1), ((('i', 'NN'), (\"'m\", 'VBP'), ('always', 'RB')), 1), ((('name', 'NN'), ('go', 'VB')), 1)]\n"
     ]
    }
   ],
   "source": [
    "# storing and printing the most common VP-chunks using the function vp_chunk_counter()\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(most_common_vp_chunks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
