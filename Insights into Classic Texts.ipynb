{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ff79ad",
   "metadata": {},
   "source": [
    "# Project 1 : Insights into Classic Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3caef",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b207551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Usuari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Importing NLTK functions for part-of-speech tagging and regex-based parsing\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "#Importing function that returns most common noun phrases and verb phrases chunks\n",
    "from chunk_counters import np_chunk_counter, vp_chunk_counter\n",
    "#Importing function that tokenizes text input firstly into sentences, then into words\n",
    "from tokenize_words import word_sentence_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# importing The Iliad from Homer found on Project Gutenberg and lowering the case for processing\n",
    "text = open('the_iliad.txt', encoding = 'utf-8').read().lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc47040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence and word tokenizing text in order to perform sentence-by-sentence parsing analysis later.\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ce7fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neptune', 'rising', 'from', 'the', 'sea', '.']\n"
     ]
    }
   ],
   "source": [
    "# storing and printing a random index of word_tokenized_text to visualize what the function returns\n",
    "\n",
    "test_word_tokenized_sentence = word_tokenized_text[50]\n",
    "print(test_word_tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d1eb1",
   "metadata": {},
   "source": [
    "## Part-of-speech Tag Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0603deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an empty list to hold part-of-speech tagged sentences from the text under analysis.\n",
    "pos_tagged_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b39a4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a for loop through each word tokenized sentence (token) and append to list of pos tagged text using nltk's pos_tag() function.\n",
    "for token in word_tokenized_text:\n",
    "    pos_tagged_text.append(pos_tag(token))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8af95d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('probability', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('and', 'CC'), ('troublesome', 'JJ'), ('test', 'NN'), (';', ':'), ('and', 'CC'), ('it', 'PRP'), ('is', 'VBZ'), ('by', 'IN'), ('this', 'DT'), ('troublesome', 'JJ'), ('standard', 'NN'), ('that', 'IN'), ('a', 'DT'), ('large', 'JJ'), ('portion', 'NN'), ('of', 'IN'), ('historical', 'JJ'), ('evidence', 'NN'), ('is', 'VBZ'), ('sifted', 'VBN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# storing and printing any random part-of-speech tagged sentence to visualize single pos-tagged sentences\n",
    "test_pos_sentence = pos_tagged_text[89]\n",
    "print(test_pos_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be552549",
   "metadata": {},
   "source": [
    "## Chunk Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4333ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a noun phrase \n",
    "#(considered here as an optional determiner, any number of adjectives and an obligatory noun, in this order) chunk grammar \n",
    "np_chunk_grammar = 'NP: {<DT>?<JJ>*<NN>}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c985dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a noun phrase RegexpParser object \n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f807a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining verb phrase (here considered as noun phrase, a verb in any form and an optional adverb, in this order) chunk grammar\n",
    "vp_chunk_grammar = 'VP: {<DT>?<JJ>*<NN><VB.*><RB>?}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ee7f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating verb phrase RegexpParser object\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "047c8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists to hold noun phrase chunked sentences and verb phrase chunked sentences\n",
    "np_chunked_text = []\n",
    "vp_chunked_text = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "162d0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a for loop through each pos-tagged sentence which chunks each sentence and appends to list \n",
    "for pos_tagged in pos_tagged_text:\n",
    "    np_chunked_text.append(np_chunk_parser.parse(pos_tagged))\n",
    "    vp_chunked_text.append(vp_chunk_parser.parse(pos_tagged))\n",
    " \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7d0e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  these/DT\n",
      "  ,/,\n",
      "  as/IN\n",
      "  my/PRP$\n",
      "  first/JJ\n",
      "  essay/NN\n",
      "  of/IN\n",
      "  arms/NNS\n",
      "  ,/,\n",
      "  i/JJ\n",
      "  won/VBD\n",
      "  ;/:\n",
      "  (VP old/JJ neleus/NN gloried/VBN)\n",
      "  in/IN\n",
      "  his/PRP$\n",
      "  conquering/VBG\n",
      "  son/NN\n",
      "  ./.)\n",
      "(S\n",
      "  these/DT\n",
      "  ,/,\n",
      "  as/IN\n",
      "  my/PRP$\n",
      "  (NP first/JJ essay/NN)\n",
      "  of/IN\n",
      "  arms/NNS\n",
      "  ,/,\n",
      "  i/JJ\n",
      "  won/VBD\n",
      "  ;/:\n",
      "  (NP old/JJ neleus/NN)\n",
      "  gloried/VBN\n",
      "  in/IN\n",
      "  his/PRP$\n",
      "  conquering/VBG\n",
      "  (NP son/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#visualizing random indexes of chunked sentences\n",
    "print(vp_chunked_text[3478])  \n",
    "print(np_chunked_text[3478])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade8062",
   "metadata": {},
   "source": [
    "## Analyze Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17916ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('hector', 'NN'),), 322), ((('i', 'NN'),), 277), ((('jove', 'NN'),), 257), ((('troy', 'NN'),), 208), ((('vain', 'NN'),), 195), ((('war', 'NN'),), 193), ((('son', 'NN'),), 170), ((('thou', 'NN'),), 158), ((('the', 'DT'), ('plain', 'NN')), 157), ((('the', 'DT'), ('field', 'NN')), 154), ((('the', 'DT'), ('ground', 'NN')), 138), ((('death', 'NN'),), 134), ((('hand', 'NN'),), 134), ((('greece', 'NN'),), 128), ((('heaven', 'NN'),), 127), ((('fate', 'NN'),), 127), ((('thee', 'NN'),), 122), ((('breast', 'NN'),), 121), ((('the', 'DT'), ('trojan', 'NN')), 120), ((('the', 'DT'), ('god', 'NN')), 119), ((('the', 'DT'), ('war', 'NN')), 117), ((('the', 'DT'), ('greeks', 'NN')), 116), ((('blood', 'NN'),), 115), ((('homer', 'NN'),), 112), ((('the', 'DT'), ('king', 'NN')), 105), ((('rage', 'NN'),), 103), ((('force', 'NN'),), 103), ((('care', 'NN'),), 99), ((('head', 'NN'),), 98), ((('man', 'NN'),), 97)]\n"
     ]
    }
   ],
   "source": [
    "# storing and printing the most common NP-chunks in order to gain insights about the book's most relevant noun phrases\n",
    "# using the function np_chunk_counter()\n",
    "\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(most_common_np_chunks)\n",
    "\n",
    "#Thanks to this process, we gain a clear insight about the novel's main protagonists, recurrent themes and symbolics as well as recurrent associations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cfdc4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(((\"'t\", 'NN'), ('is', 'VBZ')), 19), ((('i', 'NN'), ('am', 'VBP')), 11), (((\"'t\", 'NN'), ('was', 'VBD')), 11), ((('the', 'DT'), ('hero', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('know', 'VBP')), 8), ((('i', 'NN'), ('saw', 'VBD')), 8), ((('the', 'DT'), ('scene', 'NN'), ('lies', 'VBZ')), 7), ((('i', 'NN'), ('was', 'VBD')), 6), ((('confess', 'NN'), (\"'d\", 'VBD')), 6), ((('the', 'DT'), ('scene', 'NN'), ('is', 'VBZ')), 6), ((('view', 'NN'), (\"'d\", 'VBD')), 5), ((('i', 'NN'), ('felt', 'VBD')), 5), ((('i', 'NN'), ('bear', 'VBP')), 5), ((('hector', 'NN'), ('is', 'VBZ')), 5), ((('vain', 'NN'), ('was', 'VBD')), 5), ((('homer', 'NN'), ('was', 'VBD')), 4), ((('i', 'NN'), ('have', 'VBP')), 4), ((('hunger', 'NN'), ('was', 'VBD')), 4), ((('glory', 'NN'), ('lost', 'VBN')), 4), ((('i', 'NN'), ('see', 'VBP')), 4), ((('war', 'NN'), ('be', 'VB')), 4), ((('the', 'DT'), ('weapon', 'NN'), ('stood', 'VBD')), 4), ((('i', 'NN'), ('go', 'VBP')), 4), ((('the', 'DT'), ('silence', 'NN'), ('broke', 'VBD')), 4), ((('the', 'DT'), ('trojan', 'NN'), ('bands', 'VBZ')), 4), ((('father', 'NN'), ('gave', 'VBD')), 4), ((('i', 'NN'), ('deem', 'VBP')), 4), ((('minerva', 'NN'), ('repressing', 'VBG')), 3), ((('thetis', 'NN'), ('calling', 'VBG')), 3), ((('thetis', 'NN'), ('entreating', 'VBG')), 3)]\n"
     ]
    }
   ],
   "source": [
    "# storing and printing the most common VP-chunks using the function vp_chunk_counter()\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(most_common_vp_chunks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
